"""
Evaluator Agent for Enhanced Agentic-RAG.

This agent provides LLM-as-Judge evaluation capabilities to assess
the quality of generated answers against golden references.
"""

import logging
from typing import List, Dict, Any, Optional
import json
from dataclasses import dataclass

from services.openai_service import get_openai_service

logger = logging.getLogger(__name__)


@dataclass
class EvaluationResult:
    """Result of answer evaluation."""
    generated_answer: str
    golden_reference: str
    overall_score: float
    accuracy_score: float
    completeness_score: float
    relevance_score: float
    clarity_score: float
    reasoning: str
    strengths: List[str]
    weaknesses: List[str]
    suggestions: List[str]


class EvaluatorAgent:
    """
    LLM-powered agent for evaluating generated answers.
    
    This agent:
    1. Compares generated answers to golden references
    2. Provides multi-dimensional scoring (accuracy, completeness, relevance, clarity)
    3. Identifies strengths and weaknesses
    4. Offers suggestions for improvement
    """
    
    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.model_name = model_name
        self.llm_service = get_openai_service()
        self.llm = self.llm_service.get_langchain_llm()
        
        logger.info(f"EvaluatorAgent initialized with model: {model_name}")
    
    def evaluate_answer(
        self,
        generated_answer: str,
        golden_reference: str,
        original_query: str,
        context_documents: Optional[List[Dict[str, Any]]] = None
    ) -> EvaluationResult:
        """
        Evaluate a generated answer against a golden reference.
        
        Args:
            generated_answer: The answer generated by the RAG system
            golden_reference: The reference/golden answer for comparison
            original_query: The original user query
            context_documents: Context documents used for generation (optional)
            
        Returns:
            EvaluationResult with detailed evaluation
        """
        try:
            # Create evaluation prompt
            prompt = self._create_evaluation_prompt(
                generated_answer,
                golden_reference,
                original_query,
                context_documents
            )
            
            # Get LLM evaluation
            response = self.llm.invoke(prompt)
            
            # Parse evaluation response
            result = self._parse_evaluation_response(
                generated_answer,
                golden_reference,
                response.content
            )
            
            logger.info(f"Evaluated answer with overall score: {result.overall_score}")
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to evaluate answer: {e}")
            # Return fallback evaluation
            return EvaluationResult(
                generated_answer=generated_answer,
                golden_reference=golden_reference,
                overall_score=0.5,
                accuracy_score=0.5,
                completeness_score=0.5,
                relevance_score=0.5,
                clarity_score=0.5,
                reasoning="Evaluation failed due to error",
                strengths=[],
                weaknesses=["Evaluation system error"],
                suggestions=["Retry evaluation"]
            )
    
    def _create_evaluation_prompt(
        self,
        generated_answer: str,
        golden_reference: str,
        original_query: str,
        context_documents: Optional[List[Dict[str, Any]]]
    ) -> str:
        """Create the evaluation prompt."""
        
        context_info = ""
        if context_documents:
            context_info = f"\nContext Documents Used: {len(context_documents)} documents"
        
        prompt = f"""
You are an expert evaluator for RAG (Retrieval-Augmented Generation) systems. Your task is to evaluate the quality of a generated answer against a golden reference.

Original Query: {original_query}
{context_info}

Generated Answer:
{generated_answer}

Golden Reference:
{golden_reference}

Please evaluate the generated answer on the following dimensions (0.0 to 1.0 scale):

1. **Accuracy**: How factually correct is the generated answer compared to the golden reference?
2. **Completeness**: How well does the generated answer cover all aspects of the query compared to the golden reference?
3. **Relevance**: How relevant is the generated answer to the original query?
4. **Clarity**: How clear, well-structured, and easy to understand is the generated answer?

Provide your evaluation in the following JSON format:
{{
    "overall_score": 0.85,
    "accuracy_score": 0.9,
    "completeness_score": 0.8,
    "relevance_score": 0.9,
    "clarity_score": 0.8,
    "reasoning": "Detailed explanation of the evaluation",
    "strengths": ["List of strengths in the generated answer"],
    "weaknesses": ["List of weaknesses or areas for improvement"],
    "suggestions": ["Specific suggestions for improvement"]
}}

Guidelines:
- Be objective and fair in your evaluation
- Consider both content quality and presentation
- Focus on how well the answer serves the user's information need
- Provide constructive feedback for improvement
- Overall score should be a weighted average of the four dimensions

Response:
"""
        return prompt
    
    def _parse_evaluation_response(
        self,
        generated_answer: str,
        golden_reference: str,
        response: str
    ) -> EvaluationResult:
        """Parse the LLM evaluation response."""
        try:
            # Try to parse JSON response
            response_data = json.loads(response)
            
            return EvaluationResult(
                generated_answer=generated_answer,
                golden_reference=golden_reference,
                overall_score=float(response_data.get("overall_score", 0.5)),
                accuracy_score=float(response_data.get("accuracy_score", 0.5)),
                completeness_score=float(response_data.get("completeness_score", 0.5)),
                relevance_score=float(response_data.get("relevance_score", 0.5)),
                clarity_score=float(response_data.get("clarity_score", 0.5)),
                reasoning=response_data.get("reasoning", "No reasoning provided"),
                strengths=response_data.get("strengths", []),
                weaknesses=response_data.get("weaknesses", []),
                suggestions=response_data.get("suggestions", [])
            )
            
        except json.JSONDecodeError:
            # Fallback parsing if JSON is malformed
            logger.warning("Failed to parse evaluation JSON, using fallback evaluation")
            
            return EvaluationResult(
                generated_answer=generated_answer,
                golden_reference=golden_reference,
                overall_score=0.5,
                accuracy_score=0.5,
                completeness_score=0.5,
                relevance_score=0.5,
                clarity_score=0.5,
                reasoning="JSON parsing failed, using default evaluation",
                strengths=[],
                weaknesses=["Evaluation parsing error"],
                suggestions=["Retry evaluation"]
            )
    
    def batch_evaluate_answers(
        self,
        evaluation_data: List[Dict[str, Any]]
    ) -> List[EvaluationResult]:
        """
        Evaluate multiple answers in batch.
        
        Args:
            evaluation_data: List of dictionaries containing evaluation information
            
        Returns:
            List of EvaluationResult objects
        """
        results = []
        
        for data in evaluation_data:
            try:
                result = self.evaluate_answer(
                    generated_answer=data.get('generated_answer', ''),
                    golden_reference=data.get('golden_reference', ''),
                    original_query=data.get('original_query', ''),
                    context_documents=data.get('context_documents')
                )
                results.append(result)
            except Exception as e:
                logger.error(f"Failed to evaluate answer in batch: {e}")
                # Add fallback result
                results.append(EvaluationResult(
                    generated_answer=data.get('generated_answer', ''),
                    golden_reference=data.get('golden_reference', ''),
                    overall_score=0.0,
                    accuracy_score=0.0,
                    completeness_score=0.0,
                    relevance_score=0.0,
                    clarity_score=0.0,
                    reasoning=f"Batch evaluation failed: {str(e)}",
                    strengths=[],
                    weaknesses=["Batch evaluation error"],
                    suggestions=["Retry evaluation"]
                ))
        
        return results
    
    def compare_answers(
        self,
        answer1: str,
        answer2: str,
        original_query: str
    ) -> Dict[str, Any]:
        """
        Compare two answers and determine which is better.
        
        Args:
            answer1: First answer to compare
            answer2: Second answer to compare
            original_query: Original user query
            
        Returns:
            Dictionary with comparison results
        """
        try:
            prompt = f"""
You are an expert evaluator comparing two answers to the same query. Determine which answer is better and why.

Original Query: {original_query}

Answer 1:
{answer1}

Answer 2:
{answer2}

Please provide your comparison in the following JSON format:
{{
    "better_answer": 1 or 2,
    "answer1_score": 0.8,
    "answer2_score": 0.9,
    "reasoning": "Detailed explanation of why one answer is better",
    "key_differences": ["List of key differences between the answers"],
    "improvement_suggestions": ["Suggestions for improving the weaker answer"]
}}

Response:
"""
            
            response = self.llm.invoke(prompt)
            comparison_data = json.loads(response.content)
            
            return comparison_data
            
        except Exception as e:
            logger.error(f"Failed to compare answers: {e}")
            return {
                "error": f"Answer comparison failed: {str(e)}",
                "better_answer": 1,
                "answer1_score": 0.5,
                "answer2_score": 0.5,
                "reasoning": "Comparison failed due to error"
            }
    
    def get_evaluation_stats(self, results: List[EvaluationResult]) -> Dict[str, Any]:
        """Get statistics about evaluation results."""
        if not results:
            return {"error": "No results provided"}
        
        total_evaluations = len(results)
        
        # Calculate averages
        avg_overall = sum(r.overall_score for r in results) / total_evaluations
        avg_accuracy = sum(r.accuracy_score for r in results) / total_evaluations
        avg_completeness = sum(r.completeness_score for r in results) / total_evaluations
        avg_relevance = sum(r.relevance_score for r in results) / total_evaluations
        avg_clarity = sum(r.clarity_score for r in results) / total_evaluations
        
        # Count high-quality answers
        high_quality_count = sum(1 for r in results if r.overall_score >= 0.8)
        good_quality_count = sum(1 for r in results if 0.6 <= r.overall_score < 0.8)
        poor_quality_count = sum(1 for r in results if r.overall_score < 0.6)
        
        # Analyze common strengths and weaknesses
        all_strengths = [strength for r in results for strength in r.strengths]
        all_weaknesses = [weakness for r in results for weakness in r.weaknesses]
        
        return {
            "total_evaluations": total_evaluations,
            "average_scores": {
                "overall": avg_overall,
                "accuracy": avg_accuracy,
                "completeness": avg_completeness,
                "relevance": avg_relevance,
                "clarity": avg_clarity
            },
            "quality_distribution": {
                "high_quality": high_quality_count,
                "good_quality": good_quality_count,
                "poor_quality": poor_quality_count
            },
            "quality_rates": {
                "high_quality_rate": high_quality_count / total_evaluations,
                "good_quality_rate": good_quality_count / total_evaluations,
                "poor_quality_rate": poor_quality_count / total_evaluations
            },
            "common_strengths": list(set(all_strengths))[:5],  # Top 5 unique strengths
            "common_weaknesses": list(set(all_weaknesses))[:5]  # Top 5 unique weaknesses
        }
